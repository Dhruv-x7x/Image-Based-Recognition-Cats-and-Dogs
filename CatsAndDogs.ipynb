{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Cats and Dogs\n",
        "---\n",
        "Done as a part of an Unstop Course, 'Image Based Recognition'. The model was trained of Kaggle's Cats and Dogs dataset with over 24000 images. I achieved an accuracy of 92% after 25 epochs."
      ],
      "metadata": {
        "id": "nTakFoEDT0Ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "fYJnTuojTxPB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VkcT_iCZTCRM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "HjSYqz7-VYU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using kagglehub to download the cats and dogs dataset, it has 12000 images for both cats and dogs\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"karakaggle/kaggle-cat-vs-dog-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj_6L4MVV3tJ",
        "outputId": "31228d34-fb33-4f8b-afbf-9906fc214b86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/karakaggle/kaggle-cat-vs-dog-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 787M/787M [00:26<00:00, 30.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/karakaggle/kaggle-cat-vs-dog-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/karakaggle/kaggle-cat-vs-dog-dataset/versions/1/kagglecatsanddogs_3367a/PetImages\"\n",
        "print(\"Contents of the dataset directory:\", os.listdir(dataset_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwS8PIxvo3w_",
        "outputId": "39713490-3fb8-44b3-aced-e5fe849f40a7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the dataset directory: ['Cat', 'Dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "ImageDataGenerator allows for real time image augmentation as we process the images for the model.\n",
        "In other words, we can slightly alter each image using various parameters like shear, zoom, flip, brightness, sclaing, etc, to augment the data. We do this to avoid overfitting.\n",
        "This saves memory.\n",
        "`flow_from_directory` is how we apply it on the dataset directly. It automatically detects images and classes.\n",
        "'''\n",
        "train_data = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True\n",
        ")\n",
        "\n",
        "train_set = train_data.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size = (64, 64),\n",
        "    batch_size = 32,\n",
        "    class_mode = 'binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyGTyWwfT7mg",
        "outputId": "967b5a96-b8bc-4cef-b481-f20236f2fa94"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 24959 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_data.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size = (64, 64),\n",
        "    batch_size = 32,\n",
        "    class_mode = 'binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AudDqW2dvD0s",
        "outputId": "88753ed8-0ded-4f01-e2c1-d097a6a8fc55"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 24959 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN\n",
        "---\n",
        "`Sequential()` is used for straightforward networks where we have one input and output tensors. We can stack layers linearly and simply using this class.\n",
        "`add` we use this to add layers.\n",
        "`Conv2D` applies a 2D convolution. We have 32 such filters and their size is 3. This is the default setup. Input shape is also correct in our case as our images are 64x64 and colored so 64x64x3."
      ],
      "metadata": {
        "id": "YRnxvCruqtCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.models.Sequential() # init neural net\n",
        "\n",
        "# first conv layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3])) # apply convolution after deciding activation function and filter size\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2)) # apply pooling\n",
        "\n",
        "# second conv layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# flattening\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# full connection\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIeAzjYZpI2p",
        "outputId": "21761aa2-8e92-4bae-adbc-349abc61264c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the cnn\n",
        "'''\n",
        "Adam gradient descent is one of the many gradient descent techniques. We can use other metrics as well but accuracy is good enough.\n",
        "Since this is binary classification our loss reflects that.\n",
        "'''\n",
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "mk8mbrAuteo8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "---\n",
        "`cnn.fit()` based on the number of epochs, the method fits the model to our dataset iteratively."
      ],
      "metadata": {
        "id": "XLsmk4SNuJZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the model\n",
        "cnn.fit(x = train_set, validation_data = test_set, epochs = 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MQZ3aIEuLFn",
        "outputId": "46c7e96f-c53f-4ff2-f926-fd04d3bbeec8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m204/780\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:29\u001b[0m 259ms/step - accuracy: 0.5508 - loss: 0.6918"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 338ms/step - accuracy: 0.6106 - loss: 0.6497 - val_accuracy: 0.7442 - val_loss: 0.5192\n",
            "Epoch 2/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 343ms/step - accuracy: 0.7360 - loss: 0.5281 - val_accuracy: 0.7889 - val_loss: 0.4580\n",
            "Epoch 3/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 306ms/step - accuracy: 0.7681 - loss: 0.4852 - val_accuracy: 0.8046 - val_loss: 0.4219\n",
            "Epoch 4/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 308ms/step - accuracy: 0.7906 - loss: 0.4504 - val_accuracy: 0.8130 - val_loss: 0.4113\n",
            "Epoch 5/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 304ms/step - accuracy: 0.8050 - loss: 0.4198 - val_accuracy: 0.8027 - val_loss: 0.4241\n",
            "Epoch 6/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 309ms/step - accuracy: 0.8135 - loss: 0.4036 - val_accuracy: 0.8258 - val_loss: 0.3861\n",
            "Epoch 7/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 305ms/step - accuracy: 0.8216 - loss: 0.3937 - val_accuracy: 0.8189 - val_loss: 0.3962\n",
            "Epoch 8/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 325ms/step - accuracy: 0.8289 - loss: 0.3745 - val_accuracy: 0.8473 - val_loss: 0.3449\n",
            "Epoch 9/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 306ms/step - accuracy: 0.8446 - loss: 0.3556 - val_accuracy: 0.8508 - val_loss: 0.3476\n",
            "Epoch 10/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 302ms/step - accuracy: 0.8503 - loss: 0.3387 - val_accuracy: 0.8589 - val_loss: 0.3230\n",
            "Epoch 11/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 302ms/step - accuracy: 0.8506 - loss: 0.3352 - val_accuracy: 0.8853 - val_loss: 0.2764\n",
            "Epoch 12/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 327ms/step - accuracy: 0.8589 - loss: 0.3194 - val_accuracy: 0.8693 - val_loss: 0.3009\n",
            "Epoch 13/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 306ms/step - accuracy: 0.8649 - loss: 0.3095 - val_accuracy: 0.8864 - val_loss: 0.2639\n",
            "Epoch 14/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 303ms/step - accuracy: 0.8727 - loss: 0.2879 - val_accuracy: 0.8808 - val_loss: 0.2739\n",
            "Epoch 15/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 305ms/step - accuracy: 0.8818 - loss: 0.2764 - val_accuracy: 0.8894 - val_loss: 0.2602\n",
            "Epoch 16/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 303ms/step - accuracy: 0.8837 - loss: 0.2732 - val_accuracy: 0.8826 - val_loss: 0.2717\n",
            "Epoch 17/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 329ms/step - accuracy: 0.8925 - loss: 0.2569 - val_accuracy: 0.9062 - val_loss: 0.2244\n",
            "Epoch 18/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 304ms/step - accuracy: 0.8964 - loss: 0.2504 - val_accuracy: 0.9186 - val_loss: 0.1946\n",
            "Epoch 19/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 302ms/step - accuracy: 0.8944 - loss: 0.2480 - val_accuracy: 0.9113 - val_loss: 0.2115\n",
            "Epoch 20/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 303ms/step - accuracy: 0.9000 - loss: 0.2366 - val_accuracy: 0.9224 - val_loss: 0.1907\n",
            "Epoch 21/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 328ms/step - accuracy: 0.9088 - loss: 0.2232 - val_accuracy: 0.9253 - val_loss: 0.1857\n",
            "Epoch 22/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 300ms/step - accuracy: 0.9075 - loss: 0.2237 - val_accuracy: 0.9342 - val_loss: 0.1651\n",
            "Epoch 23/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 304ms/step - accuracy: 0.9102 - loss: 0.2141 - val_accuracy: 0.9357 - val_loss: 0.1643\n",
            "Epoch 24/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 329ms/step - accuracy: 0.9181 - loss: 0.2049 - val_accuracy: 0.9312 - val_loss: 0.1686\n",
            "Epoch 25/25\n",
            "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 304ms/step - accuracy: 0.9201 - loss: 0.1944 - val_accuracy: 0.9416 - val_loss: 0.1435\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c34a35b7640>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use the predict() method to test a single image\n",
        "test_image = image.load_img('/content/10.jpg',target_size=(64,64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image,axis=0)\n",
        "result = cnn.predict(test_image)\n",
        "train_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ziLmDndwkQu",
        "outputId": "04986053-6ac6-4e4b-97ae-b20c262c85de"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prediction is correct!!"
      ],
      "metadata": {
        "id": "_wN0OyGIHu-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "- Keras Documentation https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/\n",
        "- Unstop course lectures https://unstop.com/practice/machine-learning-ai-project/image-based-recognition\n",
        "- Dataset from kaggle https://www.kaggle.com/datasets/karakaggle/kaggle-cat-vs-dog-dataset\n",
        "- StackOverflow https://stackoverflow.com/questions/59953248/how-can-i-predict-single-image-in-keras-cnn-model\n",
        "- G4G https://www.geeksforgeeks.org/keras-fit-and-keras-fit_generator/"
      ],
      "metadata": {
        "id": "7Hc0CqhTZM1Z"
      }
    }
  ]
}